package ai.julie.core.model.chatcompletion.create

import ai.julie.core.model.chatcompletion.create.message.MessageRequest
import kotlinx.serialization.SerialName
import kotlinx.serialization.Serializable

@Serializable
data class CreateChatCompletion(

    /**
     * A list of messages comprising the conversation so far. Depending on the model you use,
     * different message types (modalities) are supported, like text, images, and audio.
     */
    @SerialName("messages") val messages: List<MessageRequest>,

    /**
     * Model ID used to generate the response, like gpt-4o or o3. OpenAI offers a wide range of
     * models with different capabilities, performance characteristics, and price points.
     * Refer to the model guide to browse and compare available models.
     */
    @SerialName("model") val model: String,

    /**
     * Parameters for audio output.
     * Required when audio output is requested with modalities: ["audio"].
     * [Learn more](https://platform.openai.com/docs/guides/audio).
     */
    @SerialName("audio") val audio: Audio? = null,

    /**
     * Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing
     * frequency in the text so far, decreasing the model's likelihood to repeat the same line
     * verbatim.
     */
    @SerialName("frequency_penalty") val frequencyPenalty: Double? = 0.0,

    /**
     * Deprecated in favor of tool_choice.
     *
     * Controls which (if any) function is called by the model.
     *
     * `none` means the model will not call a function and instead generates a message.
     * `auto` means the model can pick between generating a message or calling a function.
     *
     * Specifying a particular function via {"name": "my_function"} forces the model to call that
     * function.
     *
     * `none` is the default when no functions are present. auto is the default if functions are
     * present.
     */
    @Deprecated("Use tool_choice")
    @SerialName("function_call")
    val functionCall: FunctionCall? = null,

    /**
     * Deprecated in favor of tools.
     *
     * A list of functions the model may generate JSON inputs for.
     */
    @Deprecated("Use tools")
    @SerialName("functions")
    val functions: List<Function>? = null,

    /**
     * Modify the likelihood of specified tokens appearing in the completion.
     *
     * Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an
     * associated bias value from -100 to 100. Mathematically, the bias is added to the logits
     * generated by the model prior to sampling. The exact effect will vary per model, but values
     * between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100
     * should result in a ban or exclusive selection of the relevant token.
     */
    @SerialName("logit_bias") val logitBias: Map<String, Int>? = null,

    /**
     * Whether to return log probabilities of the output tokens or not. If true, returns the log
     * probabilities of each output token returned in the content of message.
     */
    @SerialName("logprobs") val logprobs: Boolean? = false,

    /**
     * An upper bound for the number of tokens that can be generated for a completion, including
     * visible output tokens and reasoning tokens.
     */
    @SerialName("max_completion_tokens") val maxCompletionTokens: Int? = null,

    /**
     * The maximum number of tokens that can be generated in the chat completion. This value can be
     * used to control costs for text generated via API.
     *
     * This value is now deprecated in favor of max_completion_tokens, and is not compatible with
     * [o-series models](https://platform.openai.com/docs/guides/reasoning).
     */
    @Deprecated("Use max_completion_tokens")
    @SerialName("max_tokens")
    val maxTokens: Int? = null,

    /**
     * Set of 16 key-value pairs that can be attached to an object. This can be useful for storing
     * additional information about the object in a structured format, and querying for objects via
     * API or the dashboard.
     *
     * Keys are strings with a maximum length of 64 characters. Values are strings with a maximum
     * length of 512 characters.
     */
    @SerialName("metadata") val metadata: Map<String, String>? = null,

    /**
     * Output types that you would like the model to generate. Most models are capable of generating
     * text, which is the default:
     *
     * ["text"]
     *
     * The gpt-4o-audio-preview model can also be used to generate audio. To request that this model
     * generate both text and audio responses, you can use:
     *
     * ["text", "audio"]
     */
    @SerialName("modalities") val modalities: List<String>? = null,

    /**
     * How many chat completion choices to generate for each input message. Note that you will be
     * charged based on the number of generated tokens across all of the choices. Keep n as 1 to
     * minimize costs.
     */
    @SerialName("n") val n: Int? = 1,

    /**
     * Whether to enable [parallel function calling](https://platform.openai.com/docs/guides/function-calling#configuring-parallel-function-calling)
     * during tool use.
     */
    @SerialName("parallel_tool_calls") val parallelToolCalls: Boolean? = true,

    /**
     * Configuration for a [Predicted Output](https://platform.openai.com/docs/guides/predicted-outputs),
     * which can greatly improve response times when large parts of the model response are known
     * ahead of time. This is most common when you are regenerating a file with only minor changes
     * to most of the content.
     */
    @SerialName("prediction") val prediction: Prediction? = null,

    /**
     * Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear
     * in the text so far, increasing the model's likelihood to talk about new topics.
     */
    @SerialName("presence_penalty") val presencePenalty: Double? = 0.0,

    /**
     * o-series models only
     *
     * Constrains effort on reasoning for reasoning models. Currently supported values are low,
     * medium, and high. Reducing reasoning effort can result in faster responses and fewer tokens
     * used on reasoning in a response.
     */
    @SerialName("reasoning_effort") val reasoningEffort: String? = "medium",

    /**
     * An object specifying the format that the model must output.
     *
     * Setting to { "type": "json_schema", "json_schema": {...} } enables Structured Outputs which
     * ensures the model will match your supplied JSON schema. Learn more in the Structured Outputs
     * guide.
     *
     * Setting to { "type": "json_object" } enables the older JSON mode, which ensures the message
     * the model generates is valid JSON. Using json_schema is preferred for models that support it.
     */
    @SerialName("response_format") val responseFormat: ResponseFormatOptionRequest? = null,

    /**
     * This feature is in Beta. If specified, our system will make a best effort to sample
     * deterministically, such that repeated requests with the same seed and parameters should
     * return the same result. Determinism is not guaranteed, and you should refer to the
     * system_fingerprint response parameter to monitor changes in the backend.
     */
    @SerialName("seed") val seed: Int? = null,

    /**
     * Specifies the latency tier to use for processing the request. This parameter is relevant for
     * customers subscribed to the scale tier service:
     *
     * If set to 'auto', and the Project is Scale tier enabled, the system will utilize scale tier
     * credits until they are exhausted.
     * If set to 'auto', and the Project is not Scale tier enabled, the request will be processed
     * using the default service tier with a lower uptime SLA and no latency guarentee.
     * If set to 'default', the request will be processed using the default service tier with a
     * lower uptime SLA and no latency guarentee.
     * If set to 'flex', the request will be processed with the Flex Processing service tier.
     * [Learn more](https://platform.openai.com/docs/guides/flex-processing).
     *
     * When not set, the default behavior is 'auto'.
     * When this parameter is set, the response body will include the service_tier utilized.
     */
    @SerialName("service_tier") val serviceTier: String? = "auto",

    /**
     * Not supported with latest reasoning models o3 and o4-mini.
     *
     * Up to 4 sequences where the API will stop generating further tokens. The returned text will
     * not contain the stop sequence.
     */
    @SerialName("stop") val stop: StopParameter = StopParameter.None,

    /**
     * Whether or not to store the output of this chat completion request for use in our model
     * distillation or evals products.
     */
    @SerialName("store") val store: Boolean? = false,

    /**
     * If set to true, the model response data will be streamed to the client as it is generated
     * using server-sent events. See the Streaming section below for more information, along with
     * the streaming responses guide for more information on how to handle the streaming events.
     */
    @SerialName("stream") val stream: Boolean? = false,

    /**
     * Options for streaming response. Only set this when you set stream: true.
     */
    @SerialName("stream_options") val streamOptions: StreamOptions? = null,

    /**
     * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the
     * output more random, while lower values like 0.2 will make it more focused and deterministic.
     * We generally recommend altering this or top_p but not both.
     */
    @SerialName("temperature") val temperature: Double? = 1.0,

    /**
     * Controls which (if any) tool is called by the model. none means the model will not call any
     * tool and instead generates a message. auto means the model can pick between generating a
     * message or calling one or more tools. required means the model must call one or more tools.
     * Specifying a particular tool via {"type": "function", "function": {"name": "my_function"}}
     * forces the model to call that tool.
     *
     * `none` is the default when no tools are present. auto is the default if tools are present.
     */
    @SerialName("tool_choice") val toolChoice: ToolChoiceOptionRequest? = null,

    /**
     * A list of tools the model may call. Currently, only functions are supported as a tool.
     * Use this to provide a list of functions the model may generate JSON inputs for. A max of 128
     * functions are supported.
     */
    @SerialName("tools") val tools: List<ToolRequest>? = null,

    /**
     * An integer between 0 and 20 specifying the number of most likely tokens to return at each
     * token position, each with an associated log probability. logprobs must be set to true if this
     * parameter is used.
     */
    @SerialName("top_logprobs") val topLogprobs: Int? = null,

    /**
     * An alternative to sampling with temperature, called nucleus sampling, where the model
     * considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens
     * comprising the top 10% probability mass are considered.
     *
     * We generally recommend altering this or temperature but not both.
     */
    @SerialName("top_p") val topP: Double? = 1.0,

    /**
     * A unique identifier representing your end-user, which can help OpenAI to monitor and detect
     * abuse. [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids).
     */
    @SerialName("user") val user: String? = null,

    /**
     * This tool searches the web for relevant results to use in a response. Learn more about the
     * web search tool.
     */
    @SerialName("web_search_options") val webSearchOptions: WebSearchOptionsRequest? = null
)

